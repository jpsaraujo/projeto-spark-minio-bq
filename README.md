# Multi-Cloud Data Lakehouse: Spark, MinIO & BigQuery

Este projeto demonstra um pipeline de dados completo utilizando uma arquitetura **Medallion**, integrando armazenamento de objetos local (MinIO) com processamento distribuÃ­do (Apache Spark) e Data Warehousing na nuvem (Google BigQuery).

## ğŸ—ï¸ Arquitetura
O pipeline segue o fluxo:
1. **Landing Zone (MinIO)**: IngestÃ£o de dados brutos em Parquet.
2. **Silver Layer (MinIO)**: Processamento e agregaÃ§Ã£o de mÃ©tricas via PySpark.
3. **Gold Layer (BigQuery)**: ExportaÃ§Ã£o final via BigQuery Storage Write API para anÃ¡lise e BI.

## ğŸ› ï¸ Tecnologias
* **Linguagem:** Python (PySpark)
* **Storage Local:** MinIO (S3-compatible)
* **Processing:** Apache Spark 3.4.1 (Dockerized)
* **Cloud DW:** Google BigQuery
* **Infra:** Docker & Docker Compose

## ğŸš€ Como Executar

### 1. PreparaÃ§Ã£o do Ambiente
```bash
# Subir infraestrutura (MinIO + Spark Cluster)
docker-compose up -d

# Instalar dependÃªncias locais (opcional para desenvolvimento)
pip install -r requirements.txt
